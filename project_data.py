# -*- coding: utf-8 -*-
"""Project_Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j3O6SuUZ5ywC_Z6U2su-AIFEE0o3lppe
"""

pip install wget

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import wget
import cv2
from sklearn.preprocessing import MinMaxScaler
import webbrowser
from datetime import datetime
import math
from google.colab import drive
drive.mount('/content/gdrive')

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

data1 = pd.read_csv('/content/gdrive/My Drive/project/hashmasks.csv', usecols=[4,9,12,13,17,19,22,95,96,125,126])
data2 = pd.read_csv('/content/gdrive/My Drive/project/axie.csv', usecols=[4,9,12,13,17,19,22,95,96,125,126])
data3 = pd.read_csv('/content/gdrive/My Drive/project/ether_cards.csv', usecols=[4,9,12,13,17,19,22,95,96,125,126])
# data4 = pd.read_csv('/content/gdrive/My Drive/project/makers.csv', usecols=[3,8,11,12,16,18,21,94,95,124,125])
data5 = pd.read_csv('/content/gdrive/My Drive/project/rarible.csv', usecols=[4,9,12,13,17,19,22,95,96,125,126])
data6 = pd.read_csv('/content/gdrive/My Drive/project/result.csv', usecols=[4,9,12,13,17,19,22,95,96,125,126])
data7 = pd.read_csv('/content/gdrive/My Drive/project/satoshi_faces.csv', usecols=[4,9,12,13,17,19,22,95,96,125,126])

# c = pd.concat([data1, data2, data3, data4, data5, data6, data7])
x = data1.append(data2, ignore_index=True)
x = x.append(data3, ignore_index=True)
# x = x.append(data4, ignore_index=True)
# x = x.append(data5, ignore_index=True)
# x = x.append(data6, ignore_index=True)
x = x.append(data7, ignore_index=True)
x_original=x

prices = np.array(x_original['orders__current_price'])
updated_prices = []
for i in prices:
  updated_prices.append(float(i)/math.pow(10,18))
updated_prices = updated_prices*x_original['orders__payment_token_contract__usd_price']
# print(updated_prices)

for i in range(len(updated_prices)):
  updated_prices[i] = (format(updated_prices[i], '.2f'))

x_original['Art_Price'] = updated_prices
x_original

sorted_data  = x_original.sort_values('Art_Price', ascending=False)
no_of_bins = 10
bin_range = (len(sorted_data)/no_of_bins)
print(bin_range)

bin_arr = []
for i in range(no_of_bins):
  bin_arr.append(bin_range*i)
bin_arr.append(len(sorted_data))
print(bin_arr)

bin_no = []
for i in range(len(sorted_data)):
  for j in range(1,len(bin_arr)):
    if(bin_arr[j-1]<=i and i<=bin_arr[j]):
      bin_no.append(j)
print(bin_no)

sorted_data['Bin_no'] = bin_no

# max_art_price = max(sorted_data['Art_Price'])
# min_art_price = min(sorted_data['Art_Price'])
# diff_art_price = max_art_price - min_art_price
# print("max_art_price :",max_art_price)
# print("min_art_price :",min_art_price)
# print("diff_art_price :",diff_art_price)

# bin_arr = []
# bin_capacity = diff_art_price/no_of_bins
# num = min_art_price
# for i in range(no_of_bins):
#   bin_arr.append(bin_capacity*i)
# bin_arr.append(max_art_price)
# print(bin_arr)

# bin_no = []
# for i in range(len(sorted_data)):
#   for j in range(1,len(bin_arr)):
#     if(bin_arr[j-1]<=sorted_data['Art_Price'][i] and sorted_data['Art_Price'][i]<=bin_arr[j]):
#       bin_no.append(j)
# print(bin_no)

sorted_data=sorted_data.drop(columns=["orders__asset__image_original_url","orders__asset__description","orders__current_price","orders__current_bounty","orders__payment_token_contract__usd_price","orders__base_price","date_value"])
sorted_data.head(10)

y = np.array(x)
print(x.columns)
print(y[0][4])
l = []
for i in range(len(y)):
  l.append(y[i][4])
# print(l)
for i in range(len(l)):
  if(type(l[i])!=str):
    l[i]= '2021-04-28T10:22:01.756793'

date_value=[]
for i in range(len(l)):
  date_value.append(int(l[i][:4]+l[i][5:7]))
#print(date_value)

x['date_value']=date_value


# Recommendation based on scaled number of sales, current price, nft version, date value and current score.
# Used MinMaxScalar to scale the values between 0 and 1
scaling = MinMaxScaler()
x_scaled_df = scaling.fit_transform(x[['orders__asset__num_sales','orders__current_price']])
x_normalized_df = pd.DataFrame(x_scaled_df, columns=[['orders__asset__num_sales','orders__current_price']])
x_normalized_df.head()

x[['orders__asset__num_sales','orders__current_price']]=x_normalized_df
x.head(10)

# Creating a column named Score which is calculated based percentage of priority given to the features.
x['Score'] = x['orders__asset__num_sales'] * 0.6 + x['orders__current_price'] * 0.3 + x['orders__asset__asset_contract__nft_version'] * 0.05 + x['date_value'] * 0.05
x_scored_df = x.sort_values(['Score'], ascending=False)
x_scored_df[['orders__asset__name','orders__asset__image_original_url','orders__asset__num_sales','orders__current_price','Score']]

x_original['orders__current_price']

for i in range(len(sorted_data['orders__asset__asset_contract__schema_name'])):
  if sorted_data['orders__asset__asset_contract__schema_name'][i]=='ERC721':
    sorted_data['orders__asset__asset_contract__schema_name'][i]=721

sorted_data['orders__asset__asset_contract__schema_name']

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn import metrics
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.dummy import DummyClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression


sorted_data['date_value']=date_value
features_data=sorted_data
print(sorted_data.columns)
features_data=features_data.drop(columns=['orders__asset__image_original_url','orders__asset__name','orders__asset__description','orders__asset__asset_contract__created_date','orders__asset__asset_contract__nft_version','orders__asset__asset_contract__schema_name','orders__current_price','orders__current_bounty','orders__payment_token_contract__usd_price','orders__base_price','Bin_no'])
X, y = features_data , sorted_data['Bin_no']
np.random.seed()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=40)

Classifiers = [DecisionTreeClassifier(max_depth=10), KNeighborsClassifier(n_neighbors=1), RandomForestClassifier(max_depth=10, random_state=0), LogisticRegression(random_state=0)]
Classifiers_name = ["Decision Tree","KNeighbors Classifier","Random Forest", "Logistic Regression"]
def Accuracy (classifiers):
  print(len(X_train))
  print(len(X_test))
  print(len(y_train))
  print(len(y_test))
  acc_ar=[]
  for i in range(len(classifiers)) :
    classifiers[i] = classifiers[i].fit(X_train,y_train)
    y_pred = classifiers[i].predict(X_test)
    print(y_pred)
    y_test_1 = np.array(y_test)
    count = 0
    for j in range(len(y_test)):
      if(y_pred[j]==y_test_1[j]):
        count +=1  
    print("Classifier: ", Classifiers_name[i])    
    print("Accuracy: " ,(count*100)/len(y_test))  
    h = (count*100)/len(y_test)
    

Accuracy(Classifiers)

plt.figure(figsize=(10,5))
accuries = [78.26086956521739, 78.26086956521739, 56.52173913043478, 73.91304347826087]
ax = sns.barplot(x=Classifiers_name, y=accuries, palette='light:#5A9')

plt.xlim(-1, 4)
plt.title('Comparison of Accuracies', weight='bold')
plt.xlabel('Classifiers', weight='bold')
plt.ylabel('Accuracy', weight='bold')

plt.savefig('Accuracy.png')

# Plotting
scored_df = x.sort_values('Score', ascending=False)

plt.figure(figsize=(16,32))

ax = sns.barplot(x=scored_df['Score'].head(121), y=scored_df['orders__asset__name'].head(121), data=scored_df, palette='flare')

plt.xlim(10089, 10106)
plt.title('Weighted Feature Ranking', weight='bold')
plt.xlabel('Score', weight='bold')
plt.ylabel('NFT Name', weight='bold')

plt.savefig('scored_nfts.png')

ini_img_list = x_scored_df[['orders__asset__name','orders__asset__image_original_url','orders__asset__num_sales','orders__current_price','Score']]
ini_img_list = np.array(ini_img_list)
img_list= []
for i in range(len(ini_img_list)):
  img_list.append(ini_img_list[i][1])
print(img_list)

local_image_filename = ''
for i in img_list:
  local_image_filename = wget.download(i)
  img = cv2.imread(local_image_filename)
  img_cvt=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
  plt.imshow(img_cvt)
  plt.axis('off')
  plt.show()